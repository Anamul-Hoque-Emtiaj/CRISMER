{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10963860,"sourceType":"datasetVersion","datasetId":4146873},{"sourceId":11933990,"sourceType":"datasetVersion","datasetId":5526369}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:58:20.369096Z","iopub.execute_input":"2025-05-24T16:58:20.369318Z","iopub.status.idle":"2025-05-24T16:58:21.152071Z","shell.execute_reply.started":"2025-05-24T16:58:20.369298Z","shell.execute_reply":"2025-05-24T16:58:21.151255Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/crispr-models/aot_idt.pth\n/kaggle/input/crispr-models/aot_idt_weights_three.pth\n/kaggle/input/crispr-models/changeseq_site_param_shap_values.pkl\n/kaggle/input/crispr-models/aot_idt_three.pth\n/kaggle/input/crispr-models/best_model_cnn_siteseq.pth\n/kaggle/input/crispr-models/siteseq_xgb_shap_values.pkl\n/kaggle/input/crispr/circleseq_all.csv\n/kaggle/input/crispr/aot_idt.pth\n/kaggle/input/crispr/II4.csv\n/kaggle/input/crispr/crisot_fingerprint_encoding.pkl\n/kaggle/input/crispr/crisot_score_param.pkl\n/kaggle/input/crispr/all_off_target.csv\n/kaggle/input/crispr/siteseq_model.pth\n/kaggle/input/crispr/crisot_merged_xgb.pkl\n/kaggle/input/crispr/guideseq.csv\n/kaggle/input/crispr/changeseq.csv\n/kaggle/input/crispr/surroseq.csv\n/kaggle/input/crispr/test_genome.fa\n/kaggle/input/crispr/hek293_off_target.csv\n/kaggle/input/crispr/changeseq_model.pth\n/kaggle/input/crispr/changeseq_siteseq.csv\n/kaggle/input/crispr/k562_off_target.csv\n/kaggle/input/crispr/siteseq.csv\n/kaggle/input/crispr/circleseq_model.pth\n/kaggle/input/crispr/ttiss.csv\n/kaggle/input/crispr/HEK293t_K562_II5.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport copy\nimport pickle\nseed = 12345\n\nos.environ['PYTHONHASHSEED']=str(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:43.047642Z","iopub.execute_input":"2025-05-24T15:44:43.047970Z","iopub.status.idle":"2025-05-24T15:44:47.048385Z","shell.execute_reply.started":"2025-05-24T15:44:43.047951Z","shell.execute_reply":"2025-05-24T15:44:47.047607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.049207Z","iopub.execute_input":"2025-05-24T15:44:47.049574Z","iopub.status.idle":"2025-05-24T15:44:47.115518Z","shell.execute_reply.started":"2025-05-24T15:44:47.049552Z","shell.execute_reply":"2025-05-24T15:44:47.114591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=8):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, kernel_size=1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, kernel_size=1, bias=False)\n        \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'Kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.116542Z","iopub.execute_input":"2025-05-24T15:44:47.116860Z","iopub.status.idle":"2025-05-24T15:44:47.136908Z","shell.execute_reply.started":"2025-05-24T15:44:47.116833Z","shell.execute_reply":"2025-05-24T15:44:47.136284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiBranchConv(nn.Module):\n    def __init__(self, output_channels=16, attention=True):\n        super(MultiBranchConv, self).__init__()\n        \n        self.branch1 = nn.Conv2d(in_channels=1, out_channels=output_channels, kernel_size=(1, 16))\n        self.branch2 = nn.Conv2d(in_channels=1, out_channels=output_channels, kernel_size=(2, 16))\n        self.branch3 = nn.Conv2d(in_channels=1, out_channels=output_channels, kernel_size=(3, 16))\n        self.branch4 = nn.Conv2d(in_channels=1, out_channels=output_channels, kernel_size=(4, 16))\n        \n        self.attn = attention\n        self.ca1 = ChannelAttention(output_channels)\n        self.ca2 = ChannelAttention(output_channels)\n        self.ca3 = ChannelAttention(output_channels)\n        self.ca4 = ChannelAttention(output_channels)\n        self.sa1 = SpatialAttention(kernel_size=7)\n        self.sa2 = SpatialAttention(kernel_size=7)\n        self.sa3 = SpatialAttention(kernel_size=7)\n        self.sa4 = SpatialAttention(kernel_size=7)\n\n    def forward(self, x):\n\n        # Branch 1: No padding needed\n        out1 = F.relu(self.branch1(x))\n\n        # Branch 2: Pad to the right (end) so shape becomes (bs, 1, 24, 16)\n        x_pad2 = F.pad(x, (0, 0, 0, 1))  # Padding only at the end (on the height dimension)\n        out2 = F.relu(self.branch2(x_pad2))\n\n        # Branch 3: Pad one row at the beginning and one at the end (bs, 1, 25, 16)\n        x_pad3 = F.pad(x, (0, 0, 1, 1))  # One padding at the start and one at the end\n        out3 = F.relu(self.branch3(x_pad3))\n\n        # Branch 4: Pad two rows at the beginning and one at the end (bs, 1, 26, 16)\n        x_pad4 = F.pad(x, (0, 0, 1, 2))  # One at the start, Two at the end\n        out4 = F.relu(self.branch4(x_pad4))\n\n        # Apply attention if enabled\n        if self.attn:\n            out1 = out1 * self.ca1(out1)\n            out1 = out1 * self.sa1(out1)\n\n            out2 = out2 * self.ca2(out2)\n            out2 = out2 * self.sa2(out2)\n\n            out3 = out3 * self.ca3(out3)\n            out3 = out3 * self.sa3(out3)\n\n            out4 = out4 * self.ca4(out4)\n            out4 = out4 * self.sa4(out4)\n\n        # Remove last dimension of size 1 (from Conv2D)\n        out1 = out1.squeeze(-1)\n        out2 = out2.squeeze(-1)\n        out3 = out3.squeeze(-1)\n        out4 = out4.squeeze(-1)\n\n        # Transpose to shape (bs, 23, 16) for concatenation later\n        out1 = out1.transpose(1, 2)\n        out2 = out2.transpose(1, 2)\n        out3 = out3.transpose(1, 2)\n        out4 = out4.transpose(1, 2)\n\n        # Concatenate along the last dimension to get shape (bs, 23, 64)\n        output = torch.cat((out1, out2, out3, out4), dim=-1)\n        \n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.137493Z","iopub.execute_input":"2025-05-24T15:44:47.137656Z","iopub.status.idle":"2025-05-24T15:44:47.154493Z","shell.execute_reply.started":"2025-05-24T15:44:47.137643Z","shell.execute_reply":"2025-05-24T15:44:47.153830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\nclass CRISPRTransformerModel(nn.Module):\n    def __init__(self, config):\n        super(CRISPRTransformerModel, self).__init__()\n        \n        # Model parameters\n        self.input_dim = 64  # Original input dimension\n        self.num_layers = config.get(\"num_layers\", 2)\n        self.num_heads = config.get(\"num_heads\", 8)\n        self.dropout_prob = config[\"dropout_prob\"]\n        self.number_hidden_layers = config[\"number_hidder_layers\"]\n        self.seq_length = config.get(\"seq_length\", 23)\n        \n        \n        # Positional encoding\n        self.pos_encoder = nn.Parameter(torch.randn(1, self.seq_length, self.input_dim))\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.input_dim,\n            nhead=self.num_heads,\n            dim_feedforward=self.input_dim * 4,\n            dropout=self.dropout_prob,\n            batch_first=True,\n            norm_first=True  \n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=self.num_layers\n        )\n        \n        # Convolutional preprocessing (from original model)\n        self.conv = MultiBranchConv(attention=config[\"attn\"])\n        \n        # Hidden layers with residual connections\n        self.hidden_layers = []\n        start_size = self.seq_length*self.input_dim\n        for i in range(self.number_hidden_layers):\n            layer = nn.Sequential(\n                nn.Linear(start_size, start_size // 2),\n                nn.GELU(),  # GELU activation (often better than ReLU for transformers)\n                nn.Dropout(self.dropout_prob)\n            )\n            self.hidden_layers.append(layer)\n            start_size = start_size // 2\n        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n        \n        # Output layer with LayerNorm\n        self.output = nn.Linear(start_size, 2)\n\n    def forward(self, x, src_mask=None):\n        # Apply conv layer (keeping original preprocessing)\n        x = self.conv(x)  # Shape: [batch_size, seq_len, input_dim]\n        \n        # Add positional encoding\n        x = x + self.pos_encoder\n        \n        # Apply transformer encoder\n        x = self.transformer_encoder(x)\n        \n        x = x.view(x.size(0), -1)\n        \n        # Apply hidden layers with residual connections\n        for layer in self.hidden_layers:\n            x = layer(x)\n        \n        x = self.output(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.155154Z","iopub.execute_input":"2025-05-24T15:44:47.155388Z","iopub.status.idle":"2025-05-24T15:44:47.174360Z","shell.execute_reply.started":"2025-05-24T15:44:47.155368Z","shell.execute_reply":"2025-05-24T15:44:47.173699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nclass TrainerDataset(Dataset):\n    def __init__(self, inputs, targets):\n        self.inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n        self.targets = torch.tensor(targets, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.176655Z","iopub.execute_input":"2025-05-24T15:44:47.177292Z","iopub.status.idle":"2025-05-24T15:44:47.192574Z","shell.execute_reply.started":"2025-05-24T15:44:47.177269Z","shell.execute_reply":"2025-05-24T15:44:47.191800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tester(model, test_x, test_y):\n    test_dataset = TrainerDataset(test_x, test_y)\n    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n    model.eval()\n    results = []\n    true_labels = []\n    with torch.no_grad():\n        for test_features, test_labels in test_dataloader:\n            outputs = model(test_features.to(device)).detach().to(\"cpu\")\n            results.extend(outputs)\n            true_labels.extend(test_labels)\n    return true_labels, results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.193461Z","iopub.execute_input":"2025-05-24T15:44:47.193717Z","iopub.status.idle":"2025-05-24T15:44:47.207412Z","shell.execute_reply.started":"2025-05-24T15:44:47.193696Z","shell.execute_reply":"2025-05-24T15:44:47.206822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Stats:\n    def __init__(self):\n        self.acc = 0\n        self.pre = 0\n        self.re = 0\n        self.f1 = 0\n        self.roc = 0\n        self.prc = 0\n        self.tn = 0\n        self.fp = 0\n        self.fn = 0\n        self.tp = 0\n        self.scores=[]\n        self.results = []\n    def print(self):\n        print('Accuracy: %.4f' %self.acc)\n        print('Precision: %.4f' %self.pre)\n        print('Recall: %.4f' %self.re)\n        print('F1 Score: %.4f' %self.f1)\n        print('ROC: %.4f' %self.roc)\n        print('PR AUC: %.4f' %self.prc)\n        print(\"Confusion Matrix\")\n        print(self.tn, \"\\t\", self.fp)\n        print(self.fn, \"\\t\", self.tp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.208079Z","iopub.execute_input":"2025-05-24T15:44:47.208706Z","iopub.status.idle":"2025-05-24T15:44:47.225719Z","shell.execute_reply.started":"2025-05-24T15:44:47.208690Z","shell.execute_reply":"2025-05-24T15:44:47.225036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve, auc, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport numpy as np\nimport torch\n\ndef eval_matrices(model, test_x, test_y, debug=True, scaler=\"minmax\"):\n    true_y, results = tester(model, test_x, test_y)  # Get true labels and logits (raw model outputs)\n    \n    # Convert logits to probabilities using softmax\n    predictions = [torch.nn.functional.softmax(r, dim=0) for r in results]\n    pred_y = np.array([y[1].item() for y in predictions])  # Probabilities of class 1\n    test_y = np.array([y.item() for y in true_y])  # Convert true labels to NumPy array\n\n    # Convert raw logits to NumPy array for scaling\n    raw_logits = np.array([r[1].item() for r in results])  # Logit values for class 1\n\n    # Apply scaling to raw logits\n    if scaler == \"minmax\":\n        scaled_scores = MinMaxScaler().fit_transform(raw_logits.reshape(-1, 1)).flatten()\n    elif scaler == \"standard\":\n        scaled_scores = StandardScaler().fit_transform(raw_logits.reshape(-1, 1)).flatten()\n    else:\n        scaled_scores = raw_logits  # Use raw logits directly if no scaler is specified\n\n    # Apply threshold to classify probabilities into binary classes\n    threshold = 0.5\n    pred_y_list = (pred_y > threshold).astype(int)\n\n    # Calculate confusion matrix metrics\n    tn, fp, fn, tp = confusion_matrix(test_y, pred_y_list).ravel()\n\n    # Calculate precision-recall curve and AUC\n    precision, recall, _ = precision_recall_curve(test_y, pred_y)\n    auc_score = auc(recall, precision)\n\n    # Calculate additional statistics\n    acc = accuracy_score(test_y, pred_y_list)\n    pr = tp / (tp + fp) if (tp + fp) > 0 else -1\n    re = tp / (tp + fn) if (tp + fn) > 0 else -1\n    f1 = (2 * pr * re / (pr + re)) if (pr + re) > 0 else -1\n\n    # Create Stats object (assuming Stats is defined elsewhere)\n    stats = Stats()\n    stats.acc = acc\n    stats.pre = pr\n    stats.re = re\n    stats.f1 = f1\n    stats.roc = roc_auc_score(test_y, raw_logits)  # Use raw logits for ROC AUC\n    stats.prc = auc_score\n    stats.tn = tn\n    stats.fp = fp\n    stats.fn = fn\n    stats.tp = tp\n    stats.scores = scaled_scores  # Use scaled logits as scores\n    stats.results = results\n\n    if debug:\n        print('Accuracy: %.4f' % acc)\n        print('Precision: %.4f' % pr)\n        print('Recall: %.4f' % re)\n        print('F1 Score: %.4f' % f1)\n        print('ROC AUC: %.4f' % stats.roc)\n        print('PR AUC: %.4f' % auc_score)\n\n    return stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.226378Z","iopub.execute_input":"2025-05-24T15:44:47.226578Z","iopub.status.idle":"2025-05-24T15:44:47.829975Z","shell.execute_reply.started":"2025-05-24T15:44:47.226563Z","shell.execute_reply":"2025-05-24T15:44:47.829395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport numpy as np\nimport torch\nfrom scipy.special import softmax\nimport joblib\n\ndef getScore(model, test_x, test_y, scaler=\"softmax\",T=10):\n    \"\"\"\n    Computes scaled scores from a model without calculating additional metrics.\n\n    Args:\n        model: The trained model.\n        test_x: Test input features.\n        test_y: Test labels (binary).\n        scaler (str): Scaling method to use ('minmax', 'standard', 'softmax', or None).\n\n    Returns:\n        np.array: Scaled scores.\n    \"\"\"\n    # Get true labels and logits (raw model outputs)\n    true_y, results = tester(model, test_x, test_y)  \n    \n    # Convert raw logits to NumPy array for scaling\n    raw_logits = np.array([r[1].item() for r in results])  # Logit values for class 1\n\n    # Apply scaling to raw logits\n    if scaler == \"minmax\":\n        scaler_instance = MinMaxScaler()\n        scaled_scores = scaler_instance.fit_transform(raw_logits.reshape(-1, 1)).flatten()\n        joblib.dump(scaler_instance, \"minmax_scaler.pkl\")  # Save the scaler\n        print(\"MinMaxScaler parameters:\", scaler_instance.min_, scaler_instance.scale_)\n    elif scaler == \"standard\":\n        scaled_scores = StandardScaler().fit_transform(raw_logits.reshape(-1, 1)).flatten()\n    elif scaler == \"softmax\":\n        predictions = [torch.nn.functional.softmax(r/T, dim=0) for r in results]\n        scaled_scores = np.array([y[1].item() for y in predictions])\n        scaler_instance = MinMaxScaler()\n        # scaler_instance = joblib.load(\"minmax_scaler.pkl\")\n        scaled_scores = scaler_instance.fit_transform(scaled_scores.reshape(-1, 1)).flatten()\n        joblib.dump(scaler_instance, \"minmax_scaler.pkl\")  # Save the scaler\n        print(\"MinMaxScaler parameters:\", scaler_instance.min_, scaler_instance.scale_)\n    else:\n        scaled_scores = raw_logits  # Use raw logits directly if no scaler is specified\n\n    return scaled_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.830690Z","iopub.execute_input":"2025-05-24T15:44:47.831094Z","iopub.status.idle":"2025-05-24T15:44:47.838534Z","shell.execute_reply.started":"2025-05-24T15:44:47.831066Z","shell.execute_reply":"2025-05-24T15:44:47.837823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"model_path = '/kaggle/input/crispr-models/aot_idt_weights_three.pth'\nconfig = {\n    'num_layers': 2, \n    'num_heads': 4, \n    'number_hidder_layers': 2, \n    'dropout_prob': 0.2, \n    'batch_size': 128, \n    'epochs': 50, \n    'learning_rate': 0.001, \n    'pos_weight': 30, \n    'attn': False,\n    \"seq_length\":20\n}\nmodel = CRISPRTransformerModel(config)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(model_path))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:47.839311Z","iopub.execute_input":"2025-05-24T15:44:47.839543Z","iopub.status.idle":"2025-05-24T15:44:48.148954Z","shell.execute_reply.started":"2025-05-24T15:44:47.839527Z","shell.execute_reply":"2025-05-24T15:44:48.148322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_hot_features(df):\n    # print(\"Generating One hot encoding features...\")\n    \n    # Nucleotides and possible pairs\n    nucleotides = ['A', 'T', 'G', 'C']\n    pairs = [f'{n1}{n2}' for n1 in nucleotides for n2 in nucleotides]  # 16 possible pairs\n    \n    # Initialize the pairwise feature matrix (rows = positions, columns = 16 pairs)\n    pairwise_features = np.zeros((len(df), 20, len(pairs)))  # (samples, positions=20, pairs=16)\n    \n    # Loop through each row in the DataFrame and populate the pairwise features\n    for idx, row in df.iterrows():\n        on_seq = row['On']\n        off_seq = row['Off']\n        \n        for pos in range(20):  # Loop through positions 1 to 20\n            pair = on_seq[pos] + off_seq[pos]  # Create the pair from the same position in both sequences\n            if pair in pairs:\n                pair_idx = pairs.index(pair)  # Get the index of the pair\n                pairwise_features[idx, pos, pair_idx] = 1  # Set the feature value to 1\n    \n    # Return a DataFrame with the pairwise features\n    # Reshape to (len(df), 20, 16) as the final output\n    pairwise_features = pairwise_features[:,:20,:]\n    return pairwise_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:48.149661Z","iopub.execute_input":"2025-05-24T15:44:48.150318Z","iopub.status.idle":"2025-05-24T15:44:48.155364Z","shell.execute_reply.started":"2025-05-24T15:44:48.150299Z","shell.execute_reply":"2025-05-24T15:44:48.154827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef graphActiveRatio(scores, true_y, num_bins=50):\n    \"\"\"\n    Plots a bar chart showing the percentage of 1s in true_y for each bin of scores.\n\n    Args:\n        scores (list or np.array): Predicted scores in the range [0, 1].\n        true_y (list or np.array): Ground truth binary labels (0 or 1).\n        num_bins (int): Number of bins for dividing the scores.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    scores = np.array(scores)\n    true_y = np.array(true_y)\n\n    # Define bin edges and initialize counts\n    bins = np.linspace(0, 1, num_bins + 1)\n    bin_indices = np.digitize(scores, bins, right=True)\n\n    # Calculate the percentage of 1s for each bin\n    bin_counts = np.zeros(num_bins)\n    bin_positives = np.zeros(num_bins)\n    for i in range(num_bins):\n        bin_mask = bin_indices == i + 1\n        bin_counts[i] = np.sum(bin_mask)\n        bin_positives[i] = np.sum(true_y[bin_mask])\n\n    # Avoid division by zero\n    active_ratios = np.divide(bin_positives, bin_counts, where=bin_counts > 0, out=np.zeros_like(bin_counts))\n\n    # Plot the bar chart\n    bin_centers = (bins[:-1] + bins[1:]) / 2\n    plt.figure(figsize=(5, 4))\n    plt.bar(bin_centers, active_ratios, width=1.0 / num_bins, edgecolor='k', alpha=0.7)\n    plt.xlabel('Score Ranges')\n    plt.ylabel('ratio of active off-targets')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.savefig('efficiency_barchart.png', dpi=300)\n\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:48.156036Z","iopub.execute_input":"2025-05-24T15:44:48.156247Z","iopub.status.idle":"2025-05-24T15:44:48.170334Z","shell.execute_reply.started":"2025-05-24T15:44:48.156223Z","shell.execute_reply":"2025-05-24T15:44:48.169687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef calculate_weights(scores ,test_y, bins):\n    \"\"\"\n    Calculate weights (active ratios) for a binary classification problem.\n\n    Parameters:\n    - model: A trained binary classification model.\n    - test_x: Features for the test dataset.\n    - test_y: Labels for the test dataset.\n    - bins (list): List of bin edges for score ranges (e.g., [0, 0.1, 0.3, 0.6, 1.0]).\n\n    Returns:\n    - pd.DataFrame: A DataFrame with bin ranges, counts, and active ratios.\n    \"\"\"\n\n    # Create a DataFrame for analysis\n    data = pd.DataFrame({\n        'score': scores,\n        'Active': test_y\n    })\n\n    # Bin the scores using the provided bin edges\n    data['bin'] = pd.cut(data['score'], bins=bins, include_lowest=True)\n\n    # Calculate counts for active and inactive within each bin\n    bin_stats = (\n    data.groupby('bin', observed=False)\n    .apply(\n        lambda x: pd.Series({\n            'active_count': (x['Active'] == 1).sum(),\n            'inactive_count': (x['Active'] == 0).sum(),\n            'total_count': len(x)\n        }),\n        include_groups=False\n    )\n    .reset_index()\n)\n\n\n    # Calculate active ratio for each bin\n    bin_stats['active_ratio'] = bin_stats['active_count'] / bin_stats['total_count']\n\n    # Handle cases where total_count is 0 to avoid division by zero\n    bin_stats['active_ratio'] = bin_stats['active_ratio'].fillna(0)\n\n    return bin_stats\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:48.170944Z","iopub.execute_input":"2025-05-24T15:44:48.171166Z","iopub.status.idle":"2025-05-24T15:44:48.189211Z","shell.execute_reply.started":"2025-05-24T15:44:48.171152Z","shell.execute_reply":"2025-05-24T15:44:48.188693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Datasets dictionary\ndatasets = {\n    'Change-seq':'/kaggle/input/crispr/changeseq.csv',\n    'Site-seq': '/kaggle/input/crispr/siteseq.csv',\n    'Circle-seq': '/kaggle/input/crispr/circleseq_all.csv',\n    'Guide-seq': '/kaggle/input/crispr/guideseq.csv',\n    'Surro-seq': '/kaggle/input/crispr/surroseq.csv',\n    'TTISS': '/kaggle/input/crispr/ttiss.csv',\n}\n\n# Initialize an empty list to store DataFrames\ndataframes = []\n\nfor name, path in datasets.items():\n    df = pd.read_csv(path)\n    dataframes.append(df)\n\n# Concatenate all DataFrames into a single DataFrame\ncombined_df = pd.concat(dataframes, ignore_index=True).reset_index(drop=True)\ncombined_df = combined_df[['On', 'Off', 'Active']]\nprint(f\"Size before dropping duplicates: {combined_df.shape}\")\n\n# Drop duplicates\ndeduplicated_df = combined_df.drop_duplicates()\n\nprint(f\"Size after dropping duplicates: {deduplicated_df.shape}\")\nprint(deduplicated_df.head())\n\ndeduplicated_df = deduplicated_df.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:48.189847Z","iopub.execute_input":"2025-05-24T15:44:48.190044Z","iopub.status.idle":"2025-05-24T15:44:55.703133Z","shell.execute_reply.started":"2025-05-24T15:44:48.190029Z","shell.execute_reply":"2025-05-24T15:44:55.702489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_x = one_hot_features(deduplicated_df)\ntest_y=deduplicated_df ['Active']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:44:55.704025Z","iopub.execute_input":"2025-05-24T15:44:55.704292Z","iopub.status.idle":"2025-05-24T15:46:53.024486Z","shell.execute_reply.started":"2025-05-24T15:44:55.704267Z","shell.execute_reply":"2025-05-24T15:46:53.023925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = getScore(model, test_x, test_y, T=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:46:53.025239Z","iopub.execute_input":"2025-05-24T15:46:53.025494Z","iopub.status.idle":"2025-05-24T15:48:33.067895Z","shell.execute_reply.started":"2025-05-24T15:46:53.025468Z","shell.execute_reply":"2025-05-24T15:48:33.067283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" graphActiveRatio(scores, true_y=test_y, num_bins=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:33.068581Z","iopub.execute_input":"2025-05-24T15:48:33.068860Z","iopub.status.idle":"2025-05-24T15:48:33.769901Z","shell.execute_reply.started":"2025-05-24T15:48:33.068835Z","shell.execute_reply":"2025-05-24T15:48:33.769251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbins = [0, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8,  0.85,0.9,0.95, 1.01]\nwt = calculate_weights(scores,test_y,bins)\n# # print(wt)\nweights = wt['active_ratio']\nprint(bins)\nprint(weights.to_numpy())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:33.770736Z","iopub.execute_input":"2025-05-24T15:48:33.771045Z","iopub.status.idle":"2025-05-24T15:48:33.906862Z","shell.execute_reply.started":"2025-05-24T15:48:33.771022Z","shell.execute_reply":"2025-05-24T15:48:33.906195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nwith open(\"bin_weights.pkl\", 'wb') as file:\n    pickle.dump([bins, weights], file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:33.907479Z","iopub.execute_input":"2025-05-24T15:48:33.907704Z","iopub.status.idle":"2025-05-24T15:48:33.911557Z","shell.execute_reply.started":"2025-05-24T15:48:33.907688Z","shell.execute_reply":"2025-05-24T15:48:33.911027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}